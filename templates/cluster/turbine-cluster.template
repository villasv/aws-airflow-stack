AWSTemplateFormatVersion: 2010-09-09
Description: The Airflow cluster stack
Parameters:
  VPCID:
    Type: AWS::EC2::VPC::Id
  VPCCIDR:
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
  PublicSubnet1ID:
    Type: String
  PublicSubnet2ID:
    Type: String
  PrivateSubnet1AID:
    Type: String
  PrivateSubnet2AID:
    Type: String
  AllowedWebBlock:
    Type: String
    AllowedPattern: >-
      ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/([0-9]|[1-2][0-9]|3[0-2]))$
  WebserverPort:
    Type: String
  SchedulerInstanceType:
    Type: String
    Default: t2.micro
  WebserverInstanceType:
    Type: String
    Default: t2.micro
  WorkerInstanceType:
    Type: String
    Default: t2.small
  MinGroupSize:
    Type: Number
  MaxGroupSize:
    Type: Number
  ShrinkThreshold:
    Type: Number
  GrowthThreshold:
    Type: Number
  LoadExampleDags:
    Type: String
  LoadDefaultConn:
    Type: String
  QSS3BucketName:
    Type: String
  QSS3KeyPrefix:
    Type: String
  DbMasterUsername:
    Type: String
  DbMasterPassword:
    Type: String
    NoEcho: true
  DbEndpointAddress:
    Type: String
  TaskQueueName:
    Type: String
  EfsFileSystem:
    Type: String

Resources:

  # ------------------------ Create Cluster Instance Services---------------------------------

  SchedulerStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Join
        - /
        - - !Sub https://${QSS3BucketName}.s3.amazonaws.com
          - !Sub ${QSS3KeyPrefix}templates
          - turbine-scheduler.template
      Parameters:
        ParentStack: !Ref AWS::StackName
        IamInstanceProfile: !Ref AirflowProfile
        IamRole: !Ref AirflowRole
        ImageId: !FindInMap
          - AWSAMIRegionMap
          - !Ref AWS::Region
          - AMZNLINUX2
        InstanceType: !Ref SchedulerInstanceType
        PrivateSubnet1AID: !Ref PrivateSubnet1AID
        PrivateSubnet2AID: !Ref PrivateSubnet2AID
    DependsOn:
      - Meta

  WebserverStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Join
        - /
        - - !Sub https://${QSS3BucketName}.s3.amazonaws.com
          - !Sub ${QSS3KeyPrefix}templates
          - turbine-webserver.template
      Parameters:
        ParentStack: !Ref AWS::StackName
        VPCID: !Ref VPCID
        IngressCIDR: !Ref AllowedWebBlock
        IngressPort: !Ref WebserverPort
        IamInstanceProfile: !Ref AirflowProfile
        IamRole: !Ref AirflowRole
        ImageId: !FindInMap
          - AWSAMIRegionMap
          - !Ref AWS::Region
          - AMZNLINUX2
        InstanceType: !Ref WebserverInstanceType
        PublicSubnet1ID: !Ref PublicSubnet1ID
        PublicSubnet2ID: !Ref PublicSubnet2ID
    DependsOn:
      - Meta

  WorkerSetStack:
    Type: AWS::CloudFormation::Stack
    Properties:
      TemplateURL: !Join
        - /
        - - !Sub https://${QSS3BucketName}.s3.amazonaws.com
          - !Sub ${QSS3KeyPrefix}templates
          - turbine-workerset.template
      Parameters:
        ParentStack: !Ref AWS::StackName
        VPCID: !Ref VPCID
        LogIngressCIDR: !Ref VPCCIDR
        IamInstanceProfile: !Ref AirflowProfile
        IamRole: !Ref AirflowRole
        ImageId: !FindInMap
          - AWSAMIRegionMap
          - !Ref AWS::Region
          - AMZNLINUX2
        InstanceType: !Ref WorkerInstanceType
        PrivateSubnet1AID: !Ref PrivateSubnet1AID
        PrivateSubnet2AID: !Ref PrivateSubnet2AID
        MinSize: !Ref MinGroupSize
        MaxSize: !Ref MaxGroupSize
        GrowthThreshold: !Ref GrowthThreshold
        ShrinkThreshold: !Ref ShrinkThreshold
    DependsOn:
      - Meta

  # ---------------------- Turbine EC2 cluster instance configuration-----------------------------

  # Loaded by every cluster instance's launch configuration respectively via
  # {$ParentStack} --resource Meta
  Meta:
    Type: AWS::CloudFormation::WaitConditionHandle
    Properties: {}
    Metadata:
      AWS::CloudFormation::Init:
        configSets:
          default:
            - filesys
            - runtime
            - secrets
            - sysconf
            - migrate
            - service
            - lchooks
            - metahup
            - cdagent
        filesys:
          commands:
            mkdir:
              test: test ! -d /airflow
              command: |
                mkdir /airflow
                chown -R ec2-user /airflow
            mount:
              test: test ! -d /mnt/efs
              command: !Sub |
                mkdir /mnt/efs
                fspec="${EfsFileSystem}.efs.${AWS::Region}.amazonaws.com:/"
                param="nfsvers=4.1,rsize=1048576,wsize=1048576"
                param="$param,hard,timeo=600,retrans=2,noresvport"
                echo "$fspec /mnt/efs nfs $param,_netdev 0 0" >> /etc/fstab
                mount /mnt/efs && chown -R ec2-user /mnt/efs
        runtime:
          packages:
            yum:
              git: []
              gcc: []
              gcc-c++: []
              jq: []
              lapack-devel: []
              libcurl-devel: []
              libxml2-devel: []
              libxslt-devel: []
              openssl-devel: []
              postgresql-devel: []
              python3: []
              python3-devel: []
              python3-pip: []
              python3-wheel: []
          commands:
            install:
              command: |
                PYCURL_SSL_LIBRARY=openssl pip3 install \
                  --no-cache-dir --compile --ignore-installed \
                  pycurl
                SLUGIFY_USES_TEXT_UNIDECODE=yes pip3 install \
                  celery[sqs] \
                  apache-airflow[celery,postgres,s3,crypto]==1.10.4
        secrets:
          commands:
            generate:
              command: !Sub |
                export $(cat /etc/environment | xargs)

                if [ "$TURBINE_MACHINE" != "SCHEDULER" ]; then
                  echo "Secret generation reserved for the scheduler"
                  exit 0
                fi
                FERNET_KEY=$(aws ssm get-parameter \
                  --name ${AWS::StackName}-fernet-key \
                  --region '${AWS::Region}' \
                  --query 'Parameter.Value')
                if [ "$FERNET_KEY" = "" ]; then
                  FERNET_KEY=$(python3 -c "if True:#
                    from cryptography.fernet import Fernet
                    key = Fernet.generate_key().decode()
                    print(key)")
                  aws ssm put-parameter \
                    --name ${AWS::StackName}-fernet-key \
                    --region '${AWS::Region}' \
                    --value $FERNET_KEY \
                    --type SecureString
                fi
            retrieve:
              command: !Sub |
                while [ "$FERNET_KEY" = "" ]; do
                  echo "Waiting for Fernet key to be available..."
                  sleep 1
                  FERNET_KEY=$(aws ssm get-parameter \
                    --name ${AWS::StackName}-fernet-key \
                    --region '${AWS::Region}' \
                    --with-decryption \
                    --query 'Parameter.Value' \
                    --output text)
                done
                echo "FERNET_KEY=$FERNET_KEY" >> /etc/environment
        sysconf:
          files:
            /etc/sysconfig/airflow:
              content: !Sub
                - |
                  TURBINE_MACHINE=${!TURBINE_MACHINE}
                  AWS_DEFAULT_REGION=${AWS::Region}
                  AIRFLOW_HOME=/airflow
                  AIRFLOW__CORE__EXECUTOR=CeleryExecutor
                  AIRFLOW__CORE__FERNET_KEY=${!FERNET_KEY}
                  AIRFLOW__CORE__LOAD_EXAMPLES=${LoadExampleDags}
                  TURBINE__CORE__LOAD_DEFAULTS=${LoadDefaultConn}
                  AIRFLOW__CORE__SQL_ALCHEMY_CONN=${DbUri}
                  AIRFLOW__CORE__REMOTE_BASE_LOG_FOLDER=s3://${LogsBucket}
                  AIRFLOW__CORE__REMOTE_LOGGING=True
                  AIRFLOW__WEBSERVER__BASE_URL=http://${!HOSTNAME}:${WebserverPort}
                  AIRFLOW__WEBSERVER__WEB_SERVER_PORT=${WebserverPort}
                  AIRFLOW__CELERY__BROKER_URL=sqs://
                  AIRFLOW__CELERY__DEFAULT_QUEUE=${QueueName}
                  AIRFLOW__CELERY__RESULT_BACKEND=db+${DbUri}
                  AIRFLOW__CELERY_BROKER_TRANSPORT_OPTIONS__REGION=${AWS::Region}
                - QueueName: !Ref TaskQueueName
                  DbUri: !Join
                    - ''
                    - - postgresql://
                      - !Ref DbMasterUsername
                      - ':'
                      - !Ref DbMasterPassword
                      - '@'
                      - !Ref DbEndpointAddress
                      - /airflow
          commands:
            envsubst:
              command: |
                export $(cat /etc/environment | xargs)

                PUBLIC=$(curl -s -o /dev/null -w "%{http_code}" \
                  http://169.254.169.254/latest/meta-data/public-ipv4)
                PUB_IPV4=$(ec2-metadata -v | awk '{print $2}')
                LOC_IPV4=$(ec2-metadata -o | awk '{print $2}')
                if [ $PUBLIC = "200" ]
                then HOSTNAME=$PUB_IPV4
                else HOSTNAME=$LOC_IPV4
                fi

                echo "$(envsubst </etc/sysconfig/airflow)" > /etc/sysconfig/airflow
        migrate:
          commands:
            migration:
              command: |
                export $(cat /etc/environment | xargs)
                export $(cat /etc/sysconfig/airflow | xargs)
                if [ "$TURBINE_MACHINE" != "SCHEDULER" ]; then
                  echo "Database setup reserved for the scheduler"
                  exit 0
                fi
                if [ "$TURBINE__CORE__LOAD_DEFAULTS" == "True" ]; then
                  su -c '/usr/local/bin/airflow initdb' ec2-user
                else
                  su -c '/usr/local/bin/airflow upgradedb' ec2-user
                fi
        service:
          files:
            /usr/bin/turbine:
              mode: 755
              content: |
                #!/bin/sh
                if [ "$TURBINE_MACHINE" == "SCHEDULER" ]
                then exec airflow scheduler
                elif [ "$TURBINE_MACHINE" == "WEBSERVER" ]
                then exec airflow webserver
                elif [ "$TURBINE_MACHINE" == "WORKER" ]
                then exec airflow worker
                else echo "TURBINE_MACHINE value unknown" && exit 1
                fi
            /usr/lib/tmpfiles.d/airflow.conf:
              content: |
                D /run/airflow 0755 ec2-user ec2-user
            /usr/lib/systemd/system/airflow.service:
              content: |
                [Service]
                EnvironmentFile=/etc/sysconfig/airflow
                User=ec2-user
                Group=ec2-user
                ExecStart=/usr/bin/turbine
                Restart=always
                RestartSec=5s
                KillMode=mixed
                TimeoutStopSec=24h
                [Install]
                WantedBy=multi-user.target
            /usr/lib/systemd/system/watcher.path:
              content: |
                [Unit]
                After=airflow.service
                PartOf=airflow.service
                [Path]
                PathModified=/etc/sysconfig/airflow
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/watcher.service:
              content: |
                [Service]
                Type=oneshot
                ExecStartPre=/usr/bin/systemctl daemon-reload
                ExecStart=/usr/bin/systemctl restart airflow
          commands:
            setup:
              command: !Sub |
                HAS_DEPLOYMENT=$(aws deploy list-deployments \
                  --application-name ${AWS::StackName}-deployment-application \
                  --deployment-group ${AWS::StackName}-deployment-group \
                  --region ${AWS::Region} | \
                  jq '.deployments | has(0)')

                systemctl enable airflow.service watcher.path

                if [ "$HAS_DEPLOYMENT" = "false" ]; then
                  systemctl start airflow
                else
                  echo "Deployment pending, deferring service start"
                fi
        # Lifecycle hooks promoting graceful shutdown, delaying termination for task completion
        lchooks:
          files:
            /usr/bin/lchkill:
              mode: 755
              content: !Sub |
                #!/bin/sh
                INSTANCE_ID=$(ec2-metadata -i | awk '{print $2}')
                TERMINATE_MESSAGE="Terminating EC2 instance <$INSTANCE_ID>"
                TERMINATING=$(aws autoscaling describe-scaling-activities \
                  --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                  --max-items 100 \
                  --region '${AWS::Region}' | \
                  jq --arg TERMINATE_MESSAGE "$TERMINATE_MESSAGE" \
                  '.Activities[]
                  | select(.Description
                  | test($TERMINATE_MESSAGE)) != []')

                if [ "$TERMINATING" = "true" ]; then
                  systemctl stop airflow
                fi
            /usr/lib/systemd/system/lchkill.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchkill.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchkill
            # WORKER ONLY <- lchbeat, lchbeat.timer
            /usr/bin/lchbeat:
              mode: 755
              content: !Sub |
                #!/bin/sh
                SERVICE_STATUS=$(systemctl is-active airflow)

                if [ "$SERVICE_STATUS" = "deactivating" ]; then
                  aws autoscaling record-lifecycle-action-heartbeat \
                    --instance-id $(ec2-metadata -i | awk '{print $2}') \
                    --lifecycle-hook-name '${AWS::StackName}-scaling-lfhook' \
                    --auto-scaling-group-name '${AWS::StackName}-scaling-group' \
                    --region '${AWS::Region}'
                fi
            /usr/lib/systemd/system/lchbeat.timer:
              content: |
                [Timer]
                OnCalendar=*:0/1
                [Install]
                WantedBy=airflow.service
            /usr/lib/systemd/system/lchbeat.service:
              content: |
                [Service]
                Type=oneshot
                ExecStart=/usr/bin/lchbeat
          commands:
            setup:
              command: |
                if [ "$TURBINE_MACHINE" = "WORKER" ]; then
                  systemctl enable lchkill.timer lchbeat.timer
                fi
        metahup:
          files:
            /etc/cfn/cfn-hup.conf:
              content: !Sub |
                [main]
                stack=${AWS::StackId}
                region=${AWS::Region}
                role=${AirflowRole}
                interval=1
            /etc/cfn/hooks.d/cfn-auto-reloader.conf:
              content: !Sub |
                [cfn-auto-reloader-hook]
                triggers=post.update
                path=Resources.Meta.Metadata.AWS::CloudFormation::Init
                action=/opt/aws/bin/cfn-init -v \
                  --region ${AWS::Region} \
                  --role ${AirflowRole} \
                  --stack ${AWS::StackName} \
                  --resource Meta
                runas=root
            /lib/systemd/system/cfn-hup.service:
              content: |
                [Service]
                ExecStart=/opt/aws/bin/cfn-hup
                Restart=always
                [Install]
                WantedBy=multi-user.target
          commands:
            setup:
              command: |
                systemctl enable cfn-hup.service
                systemctl start cfn-hup.service
        cdagent:
          packages:
            yum:
              ruby: []
              wget: []
          commands:
            install:
              command: !Sub |
                wget https://aws-codedeploy-${AWS::Region}.s3.amazonaws.com/latest/install
                chmod +x ./install
                ./install auto

  # IAM Profile and role for all EC2 cluster instances
  AirflowProfile:
    Type: AWS::IAM::InstanceProfile
    Properties:
      Roles:
        - !Ref AirflowRole

  AirflowRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - ec2.amazonaws.com
            Action:
              - sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AmazonEC2RoleforSSM
      Policies:
        - PolicyName: !Sub ${AWS::StackName}-cfn-describe
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - cloudformation:DescribeStackResource
                Resource: !Sub arn:aws:cloudformation:${AWS::Region}:${AWS::AccountId}:stack/${AWS::StackName}/*
        - PolicyName: !Sub ${AWS::StackName}-ssm-rw-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - ssm:GetParameter
                  - ssm:PutParameter
                Resource:
                  - !Sub arn:aws:ssm:*:${AWS::AccountId}:*/*
        - PolicyName: !Sub ${AWS::StackName}-queue-rw-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - sqs:ListQueues
                Resource:
                  - !Sub arn:aws:sqs:*:${AWS::AccountId}:*
              - Effect: Allow
                Action:
                  - sqs:ChangeMessageVisibility
                  - sqs:DeleteMessage
                  - sqs:GetQueueAttributes
                  - sqs:GetQueueUrl
                  - sqs:ReceiveMessage
                  - sqs:SendMessage
                Resource: !Sub
                  - arn:aws:sqs:*:${AWS::AccountId}:${queue}
                  - queue: !Ref TaskQueueName
        - PolicyName: !Sub ${AWS::StackName}-deployments-r-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:Get*
                  - s3:List*
                Resource: !Sub arn:aws:s3:::${DeploymentsBucket}/*
              - Effect: Allow
                Action:
                  - codedeploy:List*
                Resource: !Sub arn:aws:codedeploy:*:${AWS::AccountId}:deploymentgroup:*
        - PolicyName: !Sub ${AWS::StackName}-logs-rw-policy
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - s3:Get*
                  - s3:Put*
                Resource: !Sub arn:aws:s3:::${LogsBucket}/*
        - PolicyName: !Sub ${AWS::StackName}-lifecycle-heartbeat
          PolicyDocument:
            Version: 2012-10-17
            Statement:
              - Effect: Allow
                Action:
                  - autoscaling:RecordLifecycleActionHeartbeat
                  - autoscaling:CompleteLifecycleAction
                Resource: !Sub arn:aws:autoscaling:*:${AWS::AccountId}:autoScalingGroup:*:*
              - Effect: Allow
                Action:
                  - autoscaling:DescribeScalingActivities
                Resource: '*'

  # --------------------- Buckets containing Logs and Deployment packages---------------------

  LogsBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
        - ServerSideEncryptionByDefault:
            SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
         BlockPublicAcls: True
         BlockPublicPolicy: True
         IgnorePublicAcls: True
         RestrictPublicBuckets: True
    DeletionPolicy: Retain

  # NOTE: Dependency from instance configuration requires Deployments Bucket to be
  # defined here instead in turbine-codedeploy.template
  DeploymentsBucket:
    Type: AWS::S3::Bucket
    Properties:
      AccessControl: Private
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256
      PublicAccessBlockConfiguration:
        BlockPublicAcls: True
        BlockPublicPolicy: True
        IgnorePublicAcls: True
        RestrictPublicBuckets: True

Outputs:
  DeploymentsBucket:
    Value: !Ref DeploymentsBucket
  SchedulerAutoScaling:
    Value: !GetAtt SchedulerStack.Outputs.AutoScalingGroup
  WebserverAutoScaling:
    Value: !GetAtt WebserverStack.Outputs.AutoScalingGroup
  WorkerSetAutoScaling:
    Value: !GetAtt WorkerSetStack.Outputs.AutoScalingGroup

Mappings:
  AWSAMIRegionMap:
    ap-northeast-1:
      AMZNLINUX2: ami-00d101850e971728d
    ap-northeast-2:
      AMZNLINUX2: ami-08ab3f7e72215fe91
    ap-south-1:
      AMZNLINUX2: ami-00e782930f1c3dbc7
    ap-southeast-1:
      AMZNLINUX2: ami-0b5a47f8865280111
    ap-southeast-2:
      AMZNLINUX2: ami-0fb7513bcdc525c3b
    ca-central-1:
      AMZNLINUX2: ami-08a9b721ecc5b0a53
    eu-central-1:
      AMZNLINUX2: ami-0ebe657bc328d4e82
    eu-west-1:
      AMZNLINUX2: ami-030dbca661d402413
    eu-west-2:
      AMZNLINUX2: ami-0009a33f033d8b7b6
    eu-west-3:
      AMZNLINUX2: ami-0ebb3a801d5fb8b9b
    sa-east-1:
      AMZNLINUX2: ami-058141e091292ecf0
    us-east-1:
      AMZNLINUX2: ami-0c6b1d09930fac512
    us-east-2:
      AMZNLINUX2: ami-0ebbf2179e615c338
    us-west-1:
      AMZNLINUX2: ami-015954d5e5548d13b
    us-west-2:
      AMZNLINUX2: ami-0cb72367e98845d43

Metadata:
  AWS::CloudFormation::Interface:
    ParameterGroups:
      - Label:
          default: Networking
        Parameters:
          - VPCID
          - VPCCIDR
          - PublicSubnet1ID
          - PublicSubnet2ID
          - PrivateSubnet1AID
          - PrivateSubnet2AID
          - AllowedWebBlock
          - WebserverPort
      - Label:
          default: Cluster Settings
        Parameters:
          - SchedulerInstanceType
          - WebserverInstanceType
          - WorkerInstanceType
          - MinGroupSize
          - MaxGroupSize
          - ShrinkThreshold
          - GrowthThreshold
          - DbMasterUsername
          - DbMasterPassword
      - Label:
          default: Airflow Config
        Parameters:
          - LoadExampleDags
          - LoadDefaultConn
